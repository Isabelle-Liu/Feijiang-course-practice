{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch function\n",
    "transform=transforms.ToTensor()\n",
    "mnist_train=torchvision.datasets.MNIST(root='/Users/isabelleliu/Desktop/code practice',train=True, download=False, transform=transform)\n",
    "mnist_test=torchvision.datasets.MNIST(root='/Users/isabelleliu/Desktop/code practice',train=False, download=False, transform=transform)\n",
    "\n",
    "#split train into train and validation\n",
    "train_set, val_set=random_split(mnist_train,[len(mnist_train)-10000,10000])\n",
    "batch_size=64\n",
    "\n",
    "#create dataloader use default function\n",
    "train_loader=DataLoader(train_set,batch_size,shuffle=True)\n",
    "val_loader=DataLoader(val_set,batch_size,shuffle=True)\n",
    "test_loader=DataLoader(mnist_test,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define whole process\n",
    "class Trainer:\n",
    "    def __init__(self, model_paths, model, optimizer, scheduler):\n",
    "        #device = torch.device('cuda:0') if use_gpu else torch.device('cpu')\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_path=model_paths #model storage path\n",
    "        self.model=model.to(self.device) #define model\n",
    "        self.optimizer=optimizer #define optimizer\n",
    "        self.scheduler=scheduler\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.model.state_dict(), self.model_path)\n",
    "\n",
    "\n",
    "    def train_step(self, data):\n",
    "        images,labels=data\n",
    "        images, labels = images.to(self.device), labels.to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        #forward\n",
    "        predicts=self.model(images)\n",
    "        loss=F.cross_entropy(predicts,labels)\n",
    "        avg_loss=torch.mean(loss)\n",
    "        avg_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "\n",
    "        return avg_loss.item()\n",
    "    \n",
    "    def train_epoch(self,datasets,epoch):\n",
    "        self.model.train()\n",
    "        for batch, data in enumerate(datasets):\n",
    "            loss=self.train_step(data)\n",
    "\n",
    "            if batch%500==0:\n",
    "                print('epoch: {}, batch: {}, loss is: {}'.format(epoch,batch,loss))\n",
    "\n",
    "\n",
    "    def train(self, train_datasets,start_epoch, end_epoch,save_path):\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        for epoch in range(start_epoch,end_epoch):\n",
    "            self.train_epoch(train_datasets,epoch)\n",
    "\n",
    "            torch.save(self.optimizer.state_dict(),'./{}/mnist_epoch{}'.format(save_path,epoch)+'.ptopt')\n",
    "            torch.save(self.model.state_dict(), './{}/mnist_epoch{}'.format(save_path,epoch)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTOPTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTOPTM,self).__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(in_channels=1, out_channels=20,kernel_size=5,stride=1,padding=2)\n",
    "        self.max_pool1=nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.conv2=nn.Conv2d(in_channels=20,out_channels=20,kernel_size=5,stride=1,padding=2)\n",
    "        self.max_pool2=nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.fc=nn.Linear(in_features=980,out_features=10)\n",
    "\n",
    "    def forward(self,inputs,labels=None):\n",
    "        x=self.conv1(inputs)\n",
    "        x=F.relu(x)\n",
    "        x=self.max_pool1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=F.relu(x)\n",
    "        x=self.max_pool2(x)\n",
    "        x=torch.flatten(x, start_dim=1)\n",
    "        x=self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: 2.302391767501831\n",
      "epoch: 0, batch: 500, loss is: 0.33742180466651917\n",
      "epoch: 1, batch: 0, loss is: 0.3419605493545532\n",
      "epoch: 1, batch: 500, loss is: 0.28651097416877747\n",
      "epoch: 2, batch: 0, loss is: 0.2055339813232422\n",
      "epoch: 2, batch: 500, loss is: 0.12934784591197968\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1024)\n",
    "epochs=3\n",
    "model_path='mnistoptim.pth'\n",
    "\n",
    "model=MNISTOPTM()\n",
    "\n",
    "total_steps=(int(50000//batch_size)+1)*epochs\n",
    "\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "#Polynomial Decay\n",
    "lr_schedule=torch.optim.lr_scheduler.LambdaLR(optimizer,lambda step:0.01*(1-step/total_steps)**0.001)\n",
    "\n",
    "trainer=Trainer(model_paths=model_path, model=model,optimizer=optimizer,scheduler=lr_schedule)\n",
    "\n",
    "trainer.train(train_datasets=train_loader, start_epoch=0, end_epoch=epochs,save_path='checkpoint')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Decay\n",
    "`PolynomialDecay` Polynomial decay is a method used to adjust the learning rate during the training process. It gradually reduces the learning rate based on a polynomial function. A lower learning rate helps the model converge to a better solution and reduces oscillation around the minimum. \n",
    "It \n",
    "\n",
    "It is not a built-in scheduler in PyTorch, but you can create a similar learning rate scheduler using the `torch.optim.lr_scheduler.LambdaLR` class.\n",
    "\n",
    "Here's an example of how to create a polynomial decay learning rate scheduler in PyTorch using `LambdaLR`:\n",
    "\n",
    "```python\n",
    "import torch.optim as optim\n",
    "\n",
    "def polynomial_decay_lr_scheduler(optimizer, initial_lr, decay_steps, end_lr, power=1.0):\n",
    "    def lr_lambda(step):\n",
    "        return ((initial_lr - end_lr) * (1 - step / decay_steps) ** power) + end_lr\n",
    "\n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Example usage\n",
    "model = ...  # Your model definition\n",
    "initial_lr = 0.01\n",
    "decay_steps = 1000\n",
    "end_lr = 0.001\n",
    "power = 1.0\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "scheduler = polynomial_decay_lr_scheduler(optimizer, initial_lr, decay_steps, end_lr, power)\n",
    "```\n",
    "\n",
    "In the example above, we define a function `polynomial_decay_lr_scheduler` that takes the optimizer, initial learning rate, decay steps, end learning rate, and the power of the polynomial decay. It then returns a `LambdaLR` scheduler with a custom lambda function that implements the desired polynomial decay.\n",
    "\n",
    "After defining the scheduler, you can use it during training by calling `scheduler.step()` after each optimizer step:\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        # Your training loop\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "```\n",
    "\n",
    "\n",
    "This will adjust the learning rate of the optimizer according to the polynomial decay schedule."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Learning Rate Scheduling Method\n",
    "Several common learning rate scheduling methods, often referred to as decay methods, help adjust the learning rate during the training process. Here are some of the most commonly used learning rate scheduling techniques:\n",
    "\n",
    "1. Step Decay: The learning rate is reduced by a constant factor after a fixed number of epochs. This method is simple to implement and allows for a manual reduction of the learning rate at predetermined intervals.\n",
    "```python\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "```\n",
    "\n",
    "2. Exponential Decay: The learning rate decreases exponentially over time. It can be expressed as `lr = initial_lr * exp(-decay_rate * current_step)`, where `initial_lr` is the initial learning rate, `decay_rate` is a constant, and `current_step` is the current training step.\n",
    "```python\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "```\n",
    "\n",
    "3. Polynomial Decay: The learning rate decreases according to a polynomial function. It can be expressed as `lr = initial_lr * (1 - current_step / decay_steps) ** power`, where `initial_lr` is the initial learning rate, `decay_steps` is the number of steps until the learning rate reaches `end_lr`, `power` is the exponent of the polynomial, and `current_step` is the current training step.\n",
    "```python\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "decay_steps = 1000\n",
    "end_lr = 0.001\n",
    "power = 1.0\n",
    "\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lambda step: (1 - step / decay_steps) ** power)\n",
    "```\n",
    "\n",
    "4. Cosine Annealing: The learning rate decreases following a cosine curve. It is inspired by simulated annealing and can help the model escape local minima. This method typically involves warm restarts, which reset the learning rate to its initial value after each annealing cycle.\n",
    "```python\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=0.001)\n",
    "```\n",
    "\n",
    "\n",
    "5. Cyclical Learning Rates: This method involves cycling the learning rate between a minimum and maximum value during training. It can help the model escape saddle points and find better minima. The learning rate often follows a triangular or sinusoidal pattern.\n",
    "```python\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "scheduler = lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.01, step_size_up=2000, mode='triangular')\n",
    "```\n",
    "\n",
    "\n",
    "6. One-cycle Policy: This is a variation of cyclical learning rates in which the learning rate starts at a lower value, increases to a maximum value, and then decreases again during training. This method is designed to combine the benefits of both high and low learning rates.\n",
    "```python\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, total_steps=5000, epochs=50, steps_per_epoch=None, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=25.0, final_div_factor=10000.0, last_epoch=-1)\n",
    "```\n",
    "\n",
    "\n",
    "7. ReduceLROnPlateau: The learning rate is reduced when a performance metric (e.g., validation loss) stops improving. This method monitors the performance metric and reduces the learning rate by a specified factor if there's no improvement for a certain number of epochs.\n",
    "```python\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, threshold=0.0001, min_lr=0.00001)\n",
    "```\n",
    "To use the schedulers in your training loop, call the `scheduler.step()` method after each optimizer step for most schedulers:\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        # Your training loop\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "```\n",
    "\n",
    "For the `ReduceLROnPlateau` scheduler, you need to provide a metric (e.g., validation loss) when calling the `scheduler.step()` method:\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        # Your training loop\n",
    "    val_loss = compute_validation_loss()\n",
    "    scheduler.step(val_loss)\n",
    "```\n",
    "\n",
    "\n",
    "These learning rate scheduling techniques are used in different situations, depending on the specific problem and model architecture. It is often necessary to experiment with various decay methods and their hyperparameters to find the best approach for a given problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, batch: 0, loss is: 0.22667242586612701\n",
      "epoch: 1, batch: 500, loss is: 0.1558067500591278\n",
      "epoch: 2, batch: 0, loss is: 0.20005211234092712\n",
      "epoch: 2, batch: 500, loss is: 0.22454290091991425\n"
     ]
    }
   ],
   "source": [
    "#continue training\n",
    "torch.manual_seed(1024)\n",
    "model_path='mnistoptim.pth'\n",
    "\n",
    "model=MNISTOPTM()\n",
    "\n",
    "total_steps=(int(50000//batch_size)+1)*epochs\n",
    "\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "#Polynomial Decay\n",
    "lr_schedule=torch.optim.lr_scheduler.LambdaLR(optimizer,lambda step:0.01*(1-step/total_steps)**0.001)\n",
    "\n",
    "params_dict=torch.load('checkpoint/mnist_epoch0.pth')\n",
    "opt_dict=torch.load('checkpoint/mnist_epoch0.ptopt')\n",
    "\n",
    "#load the param\n",
    "model.load_state_dict(params_dict)\n",
    "optimizer.load_state_dict(opt_dict)\n",
    "\n",
    "trainer=Trainer(model_paths=model_path, model=model,optimizer=optimizer,scheduler=lr_schedule)\n",
    "\n",
    "trainer.train(train_datasets=train_loader, start_epoch=1, end_epoch=epochs,save_path='checkpoint_con')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer state dict (Adam, Adagrad)\n",
    "Saving the optimizer's state along with the model is not always necessary but can be beneficial in certain situations. Here are some reasons why you might want to save the optimizer's state:\n",
    "\n",
    "1. Resuming training: If you plan to resume training from a checkpoint, having the optimizer state saved will allow you to continue the training process with the same optimizer settings and internal state (e.g., momentum, learning rate schedule) as before. This can lead to smoother convergence and better results when compared to starting with a new optimizer or a reset optimizer state.\n",
    "\n",
    "2. Warm-starting: If you want to use the pre-trained model as a starting point for training on a related task, having the optimizer state can help speed up the initial convergence. The optimizer state might contain useful information about the gradients and weight updates that can help in adapting to the new task.\n",
    "\n",
    "3. Reproducibility: Saving the optimizer state can help with reproducibility, as it allows others to continue training the model from the same checkpoint, using the same optimizer settings and internal state.\n",
    "\n",
    "4. Finetuning: In some cases, you may want to finetune the model after the main training process is complete. In this situation, having the optimizer's state saved can be beneficial to ensure a smooth finetuning process.\n",
    "\n",
    "To save the optimizer state along with the model in PyTorch, you can create a dictionary containing both the model state and optimizer state and save it using `torch.save()`:\n",
    "\n",
    "```python\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}\n",
    "torch.save(checkpoint, 'model_checkpoint.pth')\n",
    "```\n",
    "\n",
    "To load the saved states back into the model and optimizer, use the following code:\n",
    "\n",
    "```python\n",
    "checkpoint = torch.load('model_checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "```\n",
    "\n",
    "Remember to call the appropriate methods to set the model in training or evaluation mode (`model.train()` or `model.eval()`) after loading the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
