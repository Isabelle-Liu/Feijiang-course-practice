{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 784)\n",
      "y_train shape: (50000,)\n",
      "X_val shape: (10000, 784)\n",
      "y_val shape: (10000,)\n",
      "X_test shape: (10000, 784)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "X, y = mnist\n",
    "\n",
    "# Define the sizes for the train, validation, and test sets\n",
    "train_size = 50000\n",
    "val_size = 10000\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "X_train, X_remaining = X[:train_size], X[train_size:]\n",
    "y_train, y_remaining = y[:train_size], y[train_size:]\n",
    "\n",
    "X_val, X_test = X_remaining[:val_size], X_remaining[val_size:]\n",
    "y_val, y_test = y_remaining[:val_size], y_remaining[val_size:]\n",
    "train_set = [X_train, y_train]\n",
    "val_set = [X_val, y_val]\n",
    "test_set = [X_test, y_test]\n",
    "\n",
    "original_data=[train_set,val_set,test_set]\n",
    "\n",
    "# Print shapes of the resulting datasets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train set:  50000\n"
     ]
    }
   ],
   "source": [
    "imgs,labels=train_set[0], train_set[1]\n",
    "print(\"Number of train set: \", len(imgs))\n",
    "\n",
    "imgs_length=len(imgs)\n",
    "index_list=list(range(imgs_length))\n",
    "\n",
    "#random shuffle index \n",
    "random.shuffle(index_list)\n",
    "batch_size=100\n",
    "#define data generator\n",
    "\n",
    "def data_generator():\n",
    "    imgs_list=[]\n",
    "    labels_list=[]\n",
    "    for i in index_list:\n",
    "        #change data into numpy\n",
    "        img=np.array(imgs[i]).astype('float32')\n",
    "        label=np.array(labels[i]).astype('float32')\n",
    "        imgs_list.append(img)\n",
    "        labels_list.append(label)\n",
    "        if len(imgs_list)==batch_size:\n",
    "            #get a batchsize and return\n",
    "            yield np.array(imgs_list), np.array(labels_list)\n",
    "\n",
    "            #empty the list and redo batch again\n",
    "            imgs_list=[]\n",
    "            labels_list=[]\n",
    "\n",
    "    #if the remain data number less than batchsize\n",
    "    #get a mini-batch\n",
    "    if len(imgs_list)>0:\n",
    "        yield np.array(imgs_list), np.array(labels_list)\n",
    "    return data_generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yield keyword is used in a function to create a generator. A generator is a special type of iterator that allows you to iterate over a sequence of values without creating the entire sequence in memory. Instead, generators produce the values one at a time, on-the-fly, as you request them by iterating over the generator.\n",
    "\n",
    "The yield statement is used to return a value from the generator function and temporarily suspend its execution. The next time the generator is iterated, it resumes execution right after the yield statement, retaining its previous state, including local variables and execution context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打印第一个batch数据的维度,以及数据类型:\n",
      "图像维度: (100, 784), 标签维度: (100,),图像数据类型<class 'numpy.ndarray'>, 标签数据类型<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#load traindata\n",
    "train_loader=data_generator\n",
    "\n",
    "for batch_id, data in enumerate(train_loader()):\n",
    "    image_data,label_data=data\n",
    "    if batch_id==0:\n",
    "        print(\"打印第一个batch数据的维度,以及数据类型:\")\n",
    "        print(\"图像维度: {}, 标签维度: {},图像数据类型{}, 标签数据类型{}\".format(image_data.shape, label_data.shape,type(image_data),type(label_data)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valadation \n",
    "#机器校验：加入一些校验和清理数据的操作。\n",
    "#人工校验：先打印数据输出结果，观察是否是设置的格式。再从训练的结果验证数据处理和读取的有效性。\n",
    "imgs_length = len(imgs)\n",
    "assert len(imgs) == len(labels), \\\n",
    "        \"length of train_imgs({}) should be the same as train_labels({})\".format(len(imgs), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pack load data\n",
    "def load_data(mode=\"train\"):\n",
    "    if mode=='train':\n",
    "        imgs,labels=train_set[0],train_set[1]\n",
    "    elif mode=='valid':\n",
    "        imgs,labels=val_set[0],val_set[1]\n",
    "    elif mode=='test':\n",
    "        imgs,labels=test_set[0],test_set[1]\n",
    "    else:\n",
    "        raise Exception( \"mode can only be one of ['train', 'valid', 'test']\" )\n",
    "    print(\"number of the dataset\", len(imgs))\n",
    "\n",
    "    #check shuffle\n",
    "    imgs_length=len(imgs)\n",
    "    assert len(imgs)==len(labels), \\\n",
    "          \"length of train_imgs({}) should be the same as train_labels({})\".format(len(imgs), len(labels))\n",
    "    \n",
    "\n",
    "    #shuffle\n",
    "    #define the index\n",
    "    index_list=list(range(imgs_length))\n",
    "    batch_size=100\n",
    "\n",
    "    def data_generator():\n",
    "        if mode=='train':\n",
    "            random.shuffle(index_list)\n",
    "        imgs_list=[]\n",
    "        labels_list=[]\n",
    "        for i in index_list:\n",
    "            #don't forget the normalization of images\n",
    "            img=np.array(imgs[i]).astype('float32')/255\n",
    "            label=np.array(labels[i]).astype('float32')\n",
    "            imgs_list.append(img)\n",
    "            labels_list.append(label)\n",
    "\n",
    "            if len(imgs_list)==batch_size:\n",
    "                yield np.array(imgs_list),np.array(labels_list)\n",
    "                imgs_list=[]\n",
    "                labels_list=[]\n",
    "\n",
    "        if len(imgs_list)>0:\n",
    "            yield np.array(imgs_list), np.array(labels_list)\n",
    "    return data_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST,self).__init__()\n",
    "        self.fc=torch.nn.Linear(in_features=784,out_features=1)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        outputs=self.fc(inputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    model=MNIST()\n",
    "    model.train()\n",
    "    train_loader=load_data('train')\n",
    "    optimizer=torch.optim.SGD(model.parameters(),lr=0.001)\n",
    "    epoch_num=10\n",
    "    for epoch in range(epoch_num):\n",
    "        for batch, data in enumerate(train_loader()):\n",
    "            images,labels=data\n",
    "            images=torch.from_numpy(images)\n",
    "            labels=torch.from_numpy(labels)\n",
    "\n",
    "            predicts=model(images)\n",
    "\n",
    "            loss=F.mse_loss(predicts,labels)\n",
    "            avg_loss=torch.mean(loss)\n",
    "            if batch % 200 ==0:\n",
    "                print('epoch: {},batch: {}, loss is: {}'.format(epoch,batch,avg_loss))\n",
    "\n",
    "            avg_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    torch.save(model.state_dict(),'mnist.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of the dataset 50000\n",
      "epoch: 0,batch: 0, loss is: 30.331802368164062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rz/cvwxv_893p7cj9rn8qdyqxlc0000gn/T/ipykernel_53430/2022379986.py:15: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss=F.mse_loss(predicts,labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0,batch: 200, loss is: 9.318705558776855\n",
      "epoch: 0,batch: 400, loss is: 9.835793495178223\n",
      "epoch: 1,batch: 0, loss is: 8.741619110107422\n",
      "epoch: 1,batch: 200, loss is: 8.77681827545166\n",
      "epoch: 1,batch: 400, loss is: 8.861865043640137\n",
      "epoch: 2,batch: 0, loss is: 8.994017601013184\n",
      "epoch: 2,batch: 200, loss is: 8.84766674041748\n",
      "epoch: 2,batch: 400, loss is: 9.263908386230469\n",
      "epoch: 3,batch: 0, loss is: 9.576977729797363\n",
      "epoch: 3,batch: 200, loss is: 8.399606704711914\n",
      "epoch: 3,batch: 400, loss is: 8.105779647827148\n",
      "epoch: 4,batch: 0, loss is: 8.178569793701172\n",
      "epoch: 4,batch: 200, loss is: 9.53740119934082\n",
      "epoch: 4,batch: 400, loss is: 10.157859802246094\n",
      "epoch: 5,batch: 0, loss is: 9.644535064697266\n",
      "epoch: 5,batch: 200, loss is: 8.814888954162598\n",
      "epoch: 5,batch: 400, loss is: 7.527480602264404\n",
      "epoch: 6,batch: 0, loss is: 8.687040328979492\n",
      "epoch: 6,batch: 200, loss is: 8.023115158081055\n",
      "epoch: 6,batch: 400, loss is: 7.8401408195495605\n",
      "epoch: 7,batch: 0, loss is: 9.218419075012207\n",
      "epoch: 7,batch: 200, loss is: 9.362509727478027\n",
      "epoch: 7,batch: 400, loss is: 8.208453178405762\n",
      "epoch: 8,batch: 0, loss is: 8.421826362609863\n",
      "epoch: 8,batch: 200, loss is: 8.570272445678711\n",
      "epoch: 8,batch: 400, loss is: 8.31641674041748\n",
      "epoch: 9,batch: 0, loss is: 9.739884376525879\n",
      "epoch: 9,batch: 200, loss is: 9.266586303710938\n",
      "epoch: 9,batch: 400, loss is: 9.58510971069336\n"
     ]
    }
   ],
   "source": [
    "model=MNIST()\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Normalize(mean=[0.5],std=[0.5])\n",
    "\n",
    "class MNISTData(Dataset):\n",
    "    \"\"\"\n",
    "    Step 1: Inherit the torch.utils.data.Dataset class\n",
    "    \"\"\"\n",
    "    def __init__(self,data,mode='train',transform=None):\n",
    "        super().__init__()\n",
    "        self.mode=mode\n",
    "        self.transform=transform\n",
    "        #seperate train val eval\n",
    "        train_set, val_set, test_set=data\n",
    "\n",
    "        if mode=='train':\n",
    "            self.imgs,self.labels=train_set[0],train_set[1]\n",
    "        elif mode=='valid':\n",
    "            self.imgs,self.labels=val_set[0],val_set[1]\n",
    "        elif mode=='test':\n",
    "            self.imgs,self.labels=test_set[0],test_set[1]\n",
    "        else:\n",
    "            raise Exception( \"mode can only be one of ['train', 'valid', 'test']\" )\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        \"\"\"\n",
    "        Step 3: Implement the __getitem__ method, define how to get data when specifying an index\n",
    "        \"\"\"\n",
    "        data=self.imgs[index]\n",
    "        label=int(self.labels[index])\n",
    "\n",
    "        if self.transform:\n",
    "            data=torch.tensor(data,dtype=torch.float).view(1,28,28)\n",
    "            data=self.transform(data)\n",
    "        \n",
    "        label=torch.tensor(label,dtype=torch.float)\n",
    "\n",
    "        return data, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Step 4: Implement the __len__ method, return the total number of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images:  50000 test images:  10000\n"
     ]
    }
   ],
   "source": [
    "#get data\n",
    "train_dataset=MNISTData(original_data,mode='train',transform=transform)\n",
    "test_dataset=MNISTData(original_data,mode='test',transform=transform)\n",
    "\n",
    "print('train images: ', train_dataset.__len__(), 'test images: ', test_dataset.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of image:  torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "for data in train_dataset:\n",
    "    images,labels=data\n",
    "    print('shape of image: ', images.shape)\n",
    "    plt.title(str(labels))\n",
    "    plt.imshow(images[0])\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* torch.utils.data.Dataset is an abstract base class representing a dataset. It provides a standardized interface for managing data, including retrieving data samples and their corresponding labels. When you create a custom dataset, you should inherit from this class and implement the __getitem__() and __len__() methods. The __getitem__() method defines how to access a single data point (or sample) from the dataset, while the __len__() method returns the total number of samples in the dataset.\n",
    "\n",
    "* torch.utils.data.DataLoader is a utility class that wraps a Dataset object and provides an iterator to efficiently load data in parallel, manage batching, shuffling, and other data manipulation tasks. It takes a Dataset object as input, along with other optional parameters such as batch_size, shuffle, num_workers, and collate_fn. The DataLoader simplifies the process of feeding data to a model during training and evaluation.\n",
    "\n",
    "* The relationship between Dataset and DataLoader is that Dataset is responsible for managing the data and providing a standardized interface for accessing individual samples, while DataLoader is responsible for handling efficient data loading, batching, and other data manipulation tasks. You pass a Dataset object to a DataLoader to create an iterable that can be used in your training and evaluation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step num  781\n"
     ]
    }
   ],
   "source": [
    "train_loader=DataLoader(train_dataset,batch_size=64,shuffle=True,drop_last=True)\n",
    "print('step num ', len(train_loader))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上述方法，初始化了一个数据读取器 train_loader，用于加载训练数据集。在数据读取器中几个常用的字段如下：\n",
    "\n",
    "- batch_size：每批次读取样本数，示例中 batch_size=64 表示每批次读取 64 个样本。\n",
    "- shuffle：样本乱序，示例中 shuffle=True 表示在取数据时打乱样本顺序，以减少过拟合发生的可能。\n",
    "- drop_last：丢弃不完整的批次样本，示例中 drop_last=True 表示丢弃因数据集样本数不能被 batch_size 整除而产生的最后一个不完整的 batch 样本。\n",
    "- num_workers：同步/异步读取数据，通过 num_workers 来设置加载数据的子进程个数，num_workers的值设为大于0时，即开启多进程方式异步加载数据，可提升数据读取速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create model:\n",
      "train:\n",
      "Epoch:  0\n",
      "epoch 0, batch 0, loss is: 28.018875122070312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rz/cvwxv_893p7cj9rn8qdyqxlc0000gn/T/ipykernel_53430/1681703384.py:19: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss=F.mse_loss(predicts,labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 200, loss is: 8.326987266540527\n",
      "epoch 0, batch 400, loss is: 8.990245819091797\n",
      "epoch 0, batch 600, loss is: 9.548530578613281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rz/cvwxv_893p7cj9rn8qdyqxlc0000gn/T/ipykernel_53430/1681703384.py:19: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss=F.mse_loss(predicts,labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "epoch 1, batch 0, loss is: 9.476701736450195\n",
      "epoch 1, batch 200, loss is: 8.961752891540527\n",
      "epoch 1, batch 400, loss is: 10.291584968566895\n",
      "epoch 1, batch 600, loss is: 10.397298812866211\n",
      "Epoch:  2\n",
      "epoch 2, batch 0, loss is: 9.617834091186523\n",
      "epoch 2, batch 200, loss is: 7.5260443687438965\n",
      "epoch 2, batch 400, loss is: 8.223013877868652\n",
      "epoch 2, batch 600, loss is: 9.574507713317871\n"
     ]
    }
   ],
   "source": [
    "def train(mode):\n",
    "    print('train:')\n",
    "    model.train()\n",
    "    optimizer=torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    epoch_num=3\n",
    "    for epoch in range(epoch_num):\n",
    "        print('Epoch: ', epoch)\n",
    "        for batch, data in enumerate(train_loader):\n",
    "            images,labels=data\n",
    "            images=images.to(torch.float32)\n",
    "            #labels=labels.unsqueeze(1)\n",
    "            labels=labels.to(torch.float32)\n",
    "            images=torch.reshape(images, [images.shape[0], images.shape[2]*images.shape[3]])\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predicts=model(images)\n",
    "\n",
    "            loss=F.mse_loss(predicts,labels)\n",
    "            avg_loss=torch.mean(loss)\n",
    "\n",
    "            if batch%200==0:\n",
    "                print('epoch {}, batch {}, loss is: {}'.format(epoch, batch, avg_loss))\n",
    "\n",
    "            avg_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    \n",
    "    torch.save(model.state_dict(),'mnistloader.pth')\n",
    "\n",
    "print('create model:')\n",
    "model=MNIST()\n",
    "train(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader by pytorch\n",
    "#torch function\n",
    "transform=transforms.ToTensor()\n",
    "mnist_train=torchvision.datasets.MNIST(root='/Users/isabelleliu/Desktop/code practice',train=True, download=False, transform=transform)\n",
    "mnist_test=torchvision.datasets.MNIST(root='/Users/isabelleliu/Desktop/code practice',train=False, download=False, transform=transform)\n",
    "\n",
    "#split train into train and validation\n",
    "train_set, val_set=random_split(mnist_train,[len(mnist_train)-10000,10000])\n",
    "batch_size=64\n",
    "\n",
    "#create dataloader use default function\n",
    "train_loader=DataLoader(train_set,batch_size,shuffle=True)\n",
    "val_loader=DataLoader(val_set,batch_size,shuffle=True)\n",
    "test_loader=DataLoader(mnist_test,batch_size,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "\n",
    "img_path = \"head.png\"\n",
    "\n",
    "image = Image.open(img_path)\n",
    "\n",
    "# adjust_brightness对输入图像进行亮度值调整\n",
    "new_img = TF.adjust_brightness(image, 0.4)\n",
    "\n",
    "# 显示图像\n",
    "image.show()\n",
    "new_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_img = TF.adjust_hue(image, hue_factor=0.5)\n",
    "\n",
    "# 显示图像\n",
    "image.show()\n",
    "new_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adjust_hue对输入图像进行色调的调整\n",
    "transform = transforms.RandomRotation(degrees=(-90,90))\n",
    "new_img = transform(image)\n",
    "\n",
    "# 显示图像\n",
    "image.show()\n",
    "new_img.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* from torchvision.transforms import Resize\n",
    "    - transform = Resize((256, 256))\n",
    "\n",
    "* from torchvision.transforms import RandomCrop\n",
    "    - transform = RandomCrop((224, 224))\n",
    "\n",
    "* from torchvision.transforms import RandomHorizontalFlip\n",
    "    - transform = RandomHorizontalFlip(p=0.5)\n",
    "\n",
    "* from torchvision.transforms import ColorJitter\n",
    "    - transform = ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)\n",
    "\n",
    "* from torchvision.transforms.functional import rotate\n",
    "    - new_img = rotate(image, degrees=45)\n",
    "\n",
    "* from torchvision.transforms.functional import adjust_hue\n",
    "    - new_img = adjust_hue(image, hue_factor=0.5)\n",
    "\n",
    "* from torchvision.transforms.functional import adjust_brightness\n",
    "    - new_img = adjust_brightness(image, brightness_factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
