{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#device information\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#move tensor from GPU to cpu\n",
    "x = torch.tensor(1.0, device=torch.device('cpu'))\n",
    "print(x.device)  # cpu\n",
    "\n",
    "y = x.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(y.device)  # cuda:0 if a GPU is available, otherwise cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parallelism\n",
    "In deep learning, parallelism is used to distribute the training process across multiple devices, such as GPUs or TPUs, to accelerate the training time. There are two main types of parallelism: data parallelism and model parallelism.\n",
    "\n",
    "1. Data parallelism:\n",
    "Data parallelism involves splitting the input data into smaller batches and distributing them across multiple devices. Each device processes a subset of the data independently and computes the gradients for the model's parameters. The gradients are then combined (averaged) across all devices, and the model parameters are updated accordingly.\n",
    "\n",
    "Data parallelism is the most common form of parallelism in deep learning, as it is relatively easy to implement and works well for many tasks. It can lead to substantial speedup in training, especially when working with large datasets and large batch sizes.\n",
    "\n",
    "2. Model parallelism:\n",
    "Model parallelism is the process of distributing the model itself across multiple devices. This approach is particularly useful when the model is too large to fit into the memory of a single device. In model parallelism, different parts of the model, such as layers or sub-networks, are assigned to different devices. The forward and backward passes are then performed by coordinating the execution of the model's parts across the devices.\n",
    "\n",
    "Model parallelism can be more challenging to implement than data parallelism, as it requires careful partitioning of the model and synchronization of the devices during training. However, it can be an effective strategy for training very large models that cannot fit into the memory of a single device.\n",
    "\n",
    "In some cases, it is possible to combine both data parallelism and model parallelism to take advantage of their respective strengths. This is known as hybrid parallelism and can lead to even greater speedup in training times and the ability to train extremely large models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Parallelism\n",
    "Data parallelism can be implemented in variou\n",
    "s deep learning frameworks. Here, I'll show you how to do data parallelism using PyTorch.\n",
    "\n",
    "1. First, make sure that you have multiple GPUs available on your system. You can check this using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Number of GPUs available:\", torch.cuda.device_count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Next, create your model using a neural network class that inherits from `torch.nn.Module`. For example, here is a simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now, wrap your model using `torch.nn.DataParallel`. This will automatically distribute the model across multiple GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create your data loaders for training and validation data. Make sure that the batch size is set appropriately to make use of the multiple GPUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "train_dataset = MyTrainDataset()\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "val_dataset = MyValDataset()\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Finally, train your model using the standard training loop. The data parallelism will automatically handle the distribution of the data and model across the available GPUs:\n",
    "\n",
    "By following these steps, your model will be trained using data parallelism, distributing the workload across the available GPUs to speed up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate your model\n",
    "    model.eval()\n",
    "    # ... (Your validation code here)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients across each parallel model are synchronized during the backward pass\n",
    "When using data parallelism in deep learning frameworks like PyTorch, the framework automatically ensures that the gradients across each parallel model are synchronized during the backward pass.\n",
    "\n",
    "In PyTorch, when you wrap your model with `nn.DataParallel`, it creates replicas of your model on each available GPU. During the forward pass, the input batch is divided into smaller mini-batches, and each mini-batch is processed by a separate model replica on each GPU.\n",
    "\n",
    "After the forward pass, the backward pass computes the gradients for each model replica. Once the gradients are computed, PyTorch automatically averages the gradients across all model replicas and updates the original model's parameters with these averaged gradients.\n",
    "\n",
    "You don't have to worry about synchronizing gradients manually, as the framework takes care of it. Here's the example code from the previous answer for reference:\n",
    "\n",
    "```python\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to('cuda')\n",
    "```\n",
    "\n",
    "By using `nn.DataParallel(model)`, the gradient synchronization is automatically handled by PyTorch during training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallelism on cpu\n",
    "Yes, parallelism can work on CPUs, but it typically does not provide as significant performance improvements as GPUs. This is because GPUs are specifically designed for parallel processing with a large number of cores, whereas CPUs have fewer cores optimized for general-purpose computing.\n",
    "\n",
    "However, you can still utilize multiple CPU cores for parallel processing using libraries like use `torch.nn.parallel.DistributedDataParallel` or `torch.distributed`.\n",
    "\n",
    "We use `torch.distributed` to set up a distributed training environment. The `train` function initializes the distributed environment, loads the dataset with a `DistributedSampler`, creates the model, wraps it with `DistributedDataParallel`, and performs the training. The main function spawns multiple processes to run the `train` function in parallel.\n",
    "\n",
    "Please note that `DistributedDataParallel` is designed for multi-GPU training and might not be as efficient for multi-CPU training. If you plan to use multiple GPUs, consider setting the device to `cuda` and using `nccl` as the backend for `init_process_group`.\n",
    "\n",
    "the backend used for CPU data parallelism is 'gloo'. The gloo backend supports both CPU and GPU operations, but it is optimized for CPU.\n",
    "\n",
    "`torch.multiprocessing` was not used in the revised example because `torch.distributed` provides a higher-level API for distributed training with better support for multi-GPU and multi-node setups. By using `torch.distributed` along with `DistributedDataParallel`, we can handle the complexity of distributed training more easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import Process\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "# Define the model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def train(rank, world_size, num_epochs=5, batch_size=64, learning_rate=0.01):\n",
    "    # Initialize the distributed environment\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "\n",
    "    # Load the dataset\n",
    "    #rank: The rank of the current worker process (0 to world_size-1).\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_dataset = datasets.MNIST(root='/Users/isabelleliu/Desktop/code practice', train=True, transform=transform, download=True)\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    # Create the model\n",
    "    model = SimpleNet()\n",
    "\n",
    "    # Wrap the model with DistributedDataParallel\n",
    "    model = nn.parallel.DistributedDataParallel(model)\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Start training\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = nn.functional.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Rank: {rank}, Epoch: {epoch}, Batch: {batch_idx}, Loss: {loss.item()}')\n",
    "\n",
    "    # Clean up\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def main():\n",
    "    #world_size: The total number of worker processes.\n",
    "    world_size = 4\n",
    "    processes = []\n",
    "    for rank in range(world_size):\n",
    "        p = Process(target=train, args=(rank, world_size))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output sample\n",
    "\n",
    "isabelleliu@wireless-nat-inside Desktop % python distributed_training.py\n",
    "\n",
    "[W ProcessGroupGloo.cpp:725] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
    "\n",
    "[W ProcessGroupGloo.cpp:725] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
    "\n",
    "[W ProcessGroupGloo.cpp:725] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
    "\n",
    "[W ProcessGroupGloo.cpp:725] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
    "\n",
    "Rank: 2, Epoch: 0, Batch: 0, Loss: 2.477926015853882\n",
    "Rank: 1, Epoch: 0, Batch: 0, Loss: 2.477217674255371\n",
    "Rank: 0, Epoch: 0, Batch: 0, Loss: 2.4008755683898926\n",
    "Rank: 3, Epoch: 0, Batch: 0, Loss: 2.440624952316284\n",
    "Rank: 2, Epoch: 0, Batch: 100, Loss: 0.5909628868103027\n",
    "Rank: 1, Epoch: 0, Batch: 100, Loss: 0.6203895807266235\n",
    "Rank: 3, Epoch: 0, Batch: 100, Loss: 0.4222588837146759\n",
    "Rank: 0, Epoch: 0, Batch: 100, Loss: 0.6428020596504211\n",
    "Rank: 2, Epoch: 0, Batch: 200, Loss: 0.4741484820842743\n",
    "Rank: 1, Epoch: 0, Batch: 200, Loss: 0.47861358523368835\n",
    "Rank: 0, Epoch: 0, Batch: 200, Loss: 0.3796921372413635\n",
    "Rank: 3, Epoch: 0, Batch: 200, Loss: 0.39659440517425537\n",
    "Rank: 2, Epoch: 1, Batch: 0, Loss: 0.4313582479953766\n",
    "Rank: 3, Epoch: 1, Batch: 0, Loss: 0.5145975351333618\n",
    "Rank: 1, Epoch: 1, Batch: 0, Loss: 0.6627392768859863\n",
    "Rank: 0, Epoch: 1, Batch: 0, Loss: 0.45296281576156616\n",
    "Rank: 2, Epoch: 1, Batch: 100, Loss: 0.46159809827804565\n",
    "Rank: 1, Epoch: 1, Batch: 100, Loss: 0.46639543771743774\n",
    "Rank: 0, Epoch: 1, Batch: 100, Loss: 0.43611079454421997\n",
    "Rank: 3, Epoch: 1, Batch: 100, Loss: 0.2286127507686615\n",
    "Rank: 0, Epoch: 1, Batch: 200, Loss: 0.2949753701686859\n",
    "Rank: 3, Epoch: 1, Batch: 200, Loss: 0.3011402189731598\n",
    "Rank: 1, Epoch: 1, Batch: 200, Loss: 0.40933674573898315\n",
    "Rank: 2, Epoch: 1, Batch: 200, Loss: 0.3895317614078522\n",
    "Rank: 3, Epoch: 2, Batch: 0, Loss: 0.4275888502597809\n",
    "Rank: 2, Epoch: 2, Batch: 0, Loss: 0.35294681787490845\n",
    "Rank: 1, Epoch: 2, Batch: 0, Loss: 0.598163902759552\n",
    "Rank: 0, Epoch: 2, Batch: 0, Loss: 0.3804122805595398\n",
    "Rank: 3, Epoch: 2, Batch: 100, Loss: 0.18629871308803558\n",
    "Rank: 2, Epoch: 2, Batch: 100, Loss: 0.4326363801956177\n",
    "Rank: 0, Epoch: 2, Batch: 100, Loss: 0.3880370259284973\n",
    "Rank: 1, Epoch: 2, Batch: 100, Loss: 0.43369781970977783\n",
    "Rank: 3, Epoch: 2, Batch: 200, Loss: 0.26491454243659973\n",
    "Rank: 1, Epoch: 2, Batch: 200, Loss: 0.38378211855888367\n",
    "Rank: 0, Epoch: 2, Batch: 200, Loss: 0.26226669549942017\n",
    "Rank: 2, Epoch: 2, Batch: 200, Loss: 0.3595591187477112\n",
    "Rank: 2, Epoch: 3, Batch: 0, Loss: 0.3174893260002136\n",
    "Rank: 1, Epoch: 3, Batch: 0, Loss: 0.5703485608100891\n",
    "Rank: 3, Epoch: 3, Batch: 0, Loss: 0.38869717717170715\n",
    "Rank: 0, Epoch: 3, Batch: 0, Loss: 0.34726154804229736\n",
    "Rank: 2, Epoch: 3, Batch: 100, Loss: 0.41687583923339844\n",
    "Rank: 1, Epoch: 3, Batch: 100, Loss: 0.4193125367164612\n",
    "Rank: 3, Epoch: 3, Batch: 100, Loss: 0.1669737845659256\n",
    "Rank: 0, Epoch: 3, Batch: 100, Loss: 0.3657465875148773\n",
    "Rank: 2, Epoch: 3, Batch: 200, Loss: 0.3439997434616089\n",
    "Rank: 0, Epoch: 3, Batch: 200, Loss: 0.24382120370864868\n",
    "Rank: 1, Epoch: 3, Batch: 200, Loss: 0.3696056306362152\n",
    "Rank: 3, Epoch: 3, Batch: 200, Loss: 0.24463820457458496\n",
    "Rank: 2, Epoch: 4, Batch: 0, Loss: 0.2956679165363312\n",
    "Rank: 3, Epoch: 4, Batch: 0, Loss: 0.36574244499206543\n",
    "Rank: 0, Epoch: 4, Batch: 0, Loss: 0.32660651206970215\n",
    "Rank: 1, Epoch: 4, Batch: 0, Loss: 0.5530359745025635\n",
    "Rank: 1, Epoch: 4, Batch: 100, Loss: 0.41100385785102844\n",
    "Rank: 0, Epoch: 4, Batch: 100, Loss: 0.3527187705039978\n",
    "Rank: 3, Epoch: 4, Batch: 100, Loss: 0.15558141469955444\n",
    "Rank: 2, Epoch: 4, Batch: 100, Loss: 0.4053698480129242\n",
    "Rank: 3, Epoch: 4, Batch: 200, Loss: 0.23122680187225342\n",
    "Rank: 0, Epoch: 4, Batch: 200, Loss: 0.23176565766334534\n",
    "Rank: 1, Epoch: 4, Batch: 200, Loss: 0.3601338863372803\n",
    "Rank: 2, Epoch: 4, Batch: 200, Loss: 0.33446624875068665"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates a simple distributed training example using PyTorch's `DistributedDataParallel` module. The model being trained is a simple feed-forward neural network called `SimpleNet` designed for the MNIST dataset. The code can be broken down into several parts:\n",
    "\n",
    "1. `SimpleNet` class definition: Defines a simple feed-forward neural network with one fully connected (linear) layer that takes a 28x28 input (the size of MNIST images) and outputs 10 logits (one for each class).\n",
    "\n",
    "2. `train` function: This function is the main training loop for each worker process. It takes the following arguments:\n",
    "    - `rank`: The rank of the current worker process (0 to `world_size-1`).\n",
    "    - `world_size`: The total number of worker processes.\n",
    "    - `num_epochs`, `batch_size`, and `learning_rate`: Hyperparameters for the training process.\n",
    "\n",
    "   The function initializes the distributed environment using the \"gloo\" backend, loads the dataset, creates the model, and wraps it with `DistributedDataParallel`. It then defines the optimizer and starts the training loop. During training, the model's gradients are synchronized across all worker processes to perform an update. The loss is printed every 100 batches.\n",
    "\n",
    "3. `main` function: This function creates and starts separate processes for each worker (one process per worker) using the `torch.multiprocessing.Process` class. Each process runs the `train` function with its unique rank and the total number of worker processes (`world_size`). After starting all processes, the main function waits for them to finish using the `join()` method.\n",
    "\n",
    "4. The last part of the code is the `if __name__ == '__main__':` block, which calls the `main()` function when the script is run. This ensures that the distributed training is started only when the script is run as the main module, not when it is imported as a module in another script.\n",
    "\n",
    "In summary, this code demonstrates a simple distributed training example with PyTorch using the `DistributedDataParallel` module for data parallelism. The training is performed using multiple worker processes, each with a separate instance of the model. The gradients are synchronized across all worker processes during the training process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some backend intro:\n",
    "\n",
    "1. PRC: \"Process Group RPC Backend.\" RPC stands for Remote Procedure Call, which is a communication protocol that allows one program to request a service from another program over a network. In the context of PyTorch, the RPC backend is responsible for handling communication between distributed processes. The Process Group RPC Backend is built on top of the Process Group Backend, and it uses collective communication functions to implement RPC communication.\n",
    "\n",
    "2. NCCL: NVIDIA Collective Communications Library (NCCL) is a library developed by NVIDIA that provides highly optimized multi-GPU collective communication primitives. NCCL is designed to work with NVIDIA GPUs and can scale across multiple nodes. It is widely used in deep learning frameworks like PyTorch and TensorFlow to enable efficient communication between GPUs in a distributed training environment.\n",
    "\n",
    "3. Gloo: Gloo is a collective communications library that was designed to be fast, flexible, and work well with various deep learning frameworks. It provides a variety of collective operations like AllReduce, AllGather, and Broadcast, which are essential for distributed deep learning. Gloo is part of PyTorch's distributed package and can be used as a backend for CPU and GPU-based distributed training.\n",
    "\n",
    "In summary, NCCL and Gloo are both communication backends used for distributed training in deep learning frameworks like PyTorch. NCCL is optimized for NVIDIA GPUs, while Gloo is a more general-purpose library that works with both CPUs and GPUs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch and Spawn\n",
    "\n",
    "In the context of distributed computing and parallel programming, \"launch\" and \"spawn\" refer to starting new processes or threads to perform tasks concurrently. Here is a brief explanation of each term:\n",
    "\n",
    "1. Launch: Launching a process means starting a new instance of a program, typically by creating a new process or thread, to execute the program code. In the context of distributed deep learning, \"launching\" often refers to starting multiple processes, each running a copy of the same program, to perform model training in parallel. For example, in PyTorch, the `torch.distributed.launch` module allows you to launch multiple processes for distributed training easily.\n",
    "\n",
    "2. Spawn: Spawning a process is similar to launching a process. It involves creating and starting a new process or thread to execute a specific function or a part of the program code. Spawning is often used in multiprocessing libraries to initiate parallel tasks. In Python's multiprocessing library, the `Process` class's `start` method is used to spawn a new process, and the target function is specified as a constructor argument.\n",
    "\n",
    "Both \"launch\" and \"spawn\" involve creating and starting new processes or threads to perform tasks concurrently. The difference lies in the context and the libraries or tools used to initiate these parallel tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run data parallelism on multiple GPUs using PyTorch\n",
    "To run data parallelism on multiple GPUs using PyTorch, you can use the `torch.distributed.launch` module for launching and `torch.multiprocessing.spawn` for spawning processes. Here is an example of how to use both methods:\n",
    "\n",
    "1. Using `torch.distributed.launch`:\n",
    "\n",
    "First, create a Python script (e.g., `train.py`) containing your model and training code:\n",
    "\n",
    "Next, use the `torch.distributed.launch` command to run your script on multiple GPUs:\n",
    "\n",
    "Replace `NUM_GPUS_YOU_HAVE` with the number of GPUs you want to use for data parallelism.\n",
    "\n",
    "2. Using `torch.multiprocessing.spawn`:\n",
    "\n",
    "Modify your training script (e.g., `train_spawn.py`) to include the `spawn` function and run it in bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    # Set up the device and rank\n",
    "    device_id = int(os.environ['LOCAL_RANK'])\n",
    "    torch.cuda.set_device(device_id)\n",
    "    dist.init_process_group(backend='nccl')\n",
    "\n",
    "    # Create your model, loss function, and optimizer\n",
    "    model = nn.Linear(10, 10).to(device_id)\n",
    "    ddp_model = DDP(model, device_ids=[device_id])\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(10):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ddp_model(torch.randn(20, 10).to(device_id))\n",
    "        labels = torch.randn(20, 10).to(device_id)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "#bash\n",
    "python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE train.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.multiprocessing import spawn\n",
    "import os\n",
    "\n",
    "def main(rank, world_size):\n",
    "    # Set up the device and rank\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    device_id = rank\n",
    "    torch.cuda.set_device(device_id)\n",
    "\n",
    "    # ... (the rest of the code is the same as above)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = torch.cuda.device_count()  # Number of available GPUs\n",
    "    spawn(main, args=(world_size,), nprocs=world_size, join=True)\n",
    "```\n",
    "\n",
    "#bash\n",
    "python train_spawn.py                                                                                                                                                                                                 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
