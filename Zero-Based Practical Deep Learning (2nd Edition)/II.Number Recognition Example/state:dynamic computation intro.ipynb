{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two kind of graph computation\n",
    "Dynamic graph computation and static graph computation are two ways to represent and execute computation in deep learning frameworks.\n",
    "\n",
    "Dynamic Graph Computation (also called \"eager execution\" or \"define-by-run\"):\n",
    "- In dynamic graph computation, the computation graph is built and executed on-the-fly as you write the code. It is constructed and executed step-by-step for each operation.\n",
    "- This approach is more intuitive and easier to debug because the code execution closely follows the written code. You can use standard Python debugging tools, and the code behaves like a typical Python script.\n",
    "- Dynamic graph computation is used in frameworks like PyTorch, TensorFlow 2.x (by default), and PaddlePaddle's dynamic mode.\n",
    "\n",
    "Static Graph Computation (also called \"define-and-run\"):\n",
    "- In static graph computation, the computation graph is defined and optimized before execution. The graph is constructed during the \"define\" phase, and the actual computation happens later during the \"run\" phase.\n",
    "- This approach allows the framework to perform optimizations, like memory and computation reuse, which can lead to better performance. However, it can be more challenging to debug and less intuitive.\n",
    "- Static graph computation is used in frameworks like TensorFlow 1.x and PaddlePaddle's static mode.\n",
    "\n",
    "When to use each approach:\n",
    "- Use dynamic graph computation when you need an easy-to-debug and more intuitive code. This approach is beneficial during research and development when you need to iterate and change the model structure frequently.\n",
    "- Use static graph computation when performance is a priority and you have a stable model architecture. This approach is more suitable for production environments where the model structure does not change often.\n",
    "\n",
    "The main difference between these two approaches is how the computation graph is built and executed. Dynamic graph computation builds and executes the graph as you write the code, which makes it more intuitive and easier to debug. In contrast, static graph computation requires the graph to be defined first and optimized before execution, which can result in better performance but is less intuitive and more challenging to debug."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, dynamic graph computation (also called eager execution) is the default mode. You don't need to do anything special to use it. When you write PyTorch code and define your model, you're automatically using dynamic graph computation.\n",
    "\n",
    "Here's an example of defining and training a simple model using dynamic graph computation in PyTorch:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the model, optimizer, and loss function\n",
    "model = SimpleModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "# Train the model using dynamic graph computation\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"Epoch: {} Batch: {} Loss: {:.4f}\".format(epoch, batch_idx, loss.item()))\n",
    "```\n",
    "\n",
    "In this example, the computation graph is built and executed on-the-fly during training, as you define the forward pass, loss calculation, and backpropagation.\n",
    "\n",
    "PyTorch does not have a built-in static graph computation mode like TensorFlow 1.x or PaddlePaddle's static mode. However, PyTorch provides the TorchScript module, which allows you to compile your PyTorch models into an intermediate representation suitable for deployment and optimization.\n",
    "\n",
    "To use TorchScript, you need to convert your model using `torch.jit.script` or `torch.jit.trace`. Here's an example of how to trace a model using TorchScript:\n",
    "\n",
    "```python\n",
    "# Convert the model to TorchScript\n",
    "traced_model = torch.jit.trace(model, torch.randn(1, 1, 28, 28))\n",
    "\n",
    "# Save the traced model to a file\n",
    "torch.jit.save(traced_model, 'traced_model.pt')\n",
    "\n",
    "# Load the traced model from the file\n",
    "loaded_traced_model = torch.jit.load('traced_model.pt')\n",
    "\n",
    "# Use the traced model for inference\n",
    "output = loaded_traced_model(torch.randn(1, 1, 28, 28))\n",
    "```\n",
    "\n",
    "Keep in mind that TorchScript is not the same as a static graph computation mode, but it allows you to optimize your model for deployment and can provide some performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "mnist_train=torchvision.datasets.MNIST(root='/Users/isabelleliu/Desktop/code practice',train=True, download=False, transform=transform)\n",
    "mnist_test=torchvision.datasets.MNIST(root='/Users/isabelleliu/Desktop/code practice',train=False, download=False, transform=transform)\n",
    "\n",
    "#split train into train and validation\n",
    "train_set, val_set=random_split(mnist_train,[len(mnist_train)-10000,10000])\n",
    "batch_size=64\n",
    "\n",
    "#create dataloader use default function\n",
    "train_loader=DataLoader(train_set,batch_size,shuffle=True)\n",
    "val_loader=DataLoader(val_set,batch_size,shuffle=True)\n",
    "test_loader=DataLoader(mnist_test,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_id: 0, batch_id: 0, loss is: 2.3578267097473145\n",
      "epoch_id: 1, batch_id: 0, loss is: 2.2945075035095215\n",
      "epoch_id: 2, batch_id: 0, loss is: 2.3089680671691895\n",
      "epoch_id: 3, batch_id: 0, loss is: 2.298851251602173\n",
      "epoch_id: 4, batch_id: 0, loss is: 2.304023504257202\n",
      "epoch_id: 5, batch_id: 0, loss is: 2.320495367050171\n",
      "epoch_id: 6, batch_id: 0, loss is: 2.308727502822876\n",
      "epoch_id: 7, batch_id: 0, loss is: 2.296772003173828\n",
      "epoch_id: 8, batch_id: 0, loss is: 2.3020801544189453\n",
      "epoch_id: 9, batch_id: 0, loss is: 2.310842514038086\n",
      "==> Trained model saved in temp.pth\n"
     ]
    }
   ],
   "source": [
    "# Define the MNIST model\n",
    "class MNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST, self).__init__()\n",
    "        self.fc = nn.Linear(in_features=784, out_features=10)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.fc(inputs)\n",
    "        return outputs\n",
    "\n",
    "# Image normalization function\n",
    "def norm_img(img):\n",
    "    batch_size = img.shape[0]\n",
    "    img = img / 127.5 - 1\n",
    "    img = torch.reshape(img, (batch_size, 784))\n",
    "    return img\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    opt = optim.SGD(model.parameters(), lr=0.001)\n",
    "    EPOCH_NUM = 10\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        for batch_id, (data, target) in enumerate(train_loader):\n",
    "            images = norm_img(data).type(torch.float32)\n",
    "            labels = target.type(torch.int64)\n",
    "\n",
    "            # Forward pass\n",
    "            predicts = model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = torch.mean(loss)\n",
    "\n",
    "            # Print loss every 1000 batches\n",
    "            if batch_id % 1000 == 0:\n",
    "                print(\"epoch_id: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, avg_loss.item()))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "model = MNIST()\n",
    "\n",
    "train(model)\n",
    "\n",
    "torch.save(model.state_dict(), 'temp.pth')\n",
    "print(\"==> Trained model saved in temp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label of readed image is:  7\n",
      "The predicted label is:  0\n"
     ]
    }
   ],
   "source": [
    "test_image, label = mnist_test[0]\n",
    "print(\"The label of readed image is: \", label)\n",
    "\n",
    "# Convert the test image to a tensor and reshape it to [1, 784]\n",
    "test_image = test_image.view(1, -1)\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = MNIST()\n",
    "loaded_model.load_state_dict(torch.load(\"temp.pth\"))\n",
    "loaded_model.eval()\n",
    "\n",
    "# Use the loaded model for prediction\n",
    "preds = loaded_model(test_image)\n",
    "pred_label = torch.argmax(preds)\n",
    "\n",
    "# Print the prediction result\n",
    "print(\"The predicted label is: \", pred_label.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
